{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[Sequence[BaseMassage], operator.add]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQXBhJJMEl5i"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "  input: str\n",
    "  chat_history: list[BaseMessage]\n",
    "  agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "  intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JB8CeRClIsiS",
    "outputId": "d8c9d674-406c-4537-f1f4-60465e6336ba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -U google-generativeai\n",
    "%pip install google-ai-generativelanguage==0.6.15\n",
    "%pip install -U langchain-google-genai\n",
    "%pip install -U langchain-community\n",
    "%pip install -U langgraph\n",
    "%pip install -U langgraph langchain-community\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN-nEzmjOkkV"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeWDWE1ULrqK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GEMINI_API_KEY') \n",
    "os.environ['TAVILY_API_KEY'] = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CbwO2DZEIseI",
    "outputId": "54abe20b-b000-4aed-a438-68ef4e627785"
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=4)\n",
    "print(type(tool))\n",
    "print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyT_MqIoIscB"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict): \n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxpKvnjnIsZl"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_gemini)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_gemini(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geAXPdfhIsXE"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Você é um assistente de pesquisa inteligente. Use o mecanismo de busca para procurar informações. \\\n",
    "Você tem permissão para fazer múltiplas chamadas (seja em conjunto ou em sequência). \\\n",
    "Procure informações apenas quando tiver certeza do que você quer. \\\n",
    "Se precisar pesquisar alguma informação antes de fazer uma pergunta de acompanhamento, você tem permissão para fazer isso!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "abot = Agent(model, [tool], system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mermaid_code = abot.graph.get_graph().draw_mermaid()\n",
    "print(mermaid_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    \n",
    "    image_data = abot.graph.get_graph().draw_mermaid_png()\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao tentar gerar PNG do Mermaid: {e}\")\n",
    "    print(\"\\nCertifique-se de que a sua versão do LangGraph possui o método `.draw_mermaid_png()`.\")\n",
    "    print(\"Como alternativa, use `.draw_mermaid()` para obter a string e visualizar externamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npzg1Np4mi2T",
    "outputId": "4569c5a1-00f9-4b96-c699-39a26cfb66f9"
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain-tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKYliqwytb1C"
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_gemini)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_gemini(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t_call in tool_calls:\n",
    "            print(f\"Calling tool: {t_call['name']} with args: {t_call['args']}\")\n",
    "            if t_call['name'] not in self.tools:\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"\n",
    "            else:\n",
    "                result = self.tools[t_call['name']].invoke(t_call['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t_call['id'], name=t_call['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "id": "jeY5hzLst9vS",
    "outputId": "1297f5f0-7b1d-4dba-c5d7-6fc51414a5f1"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Você é um assistente de pesquisa inteligente. Use o mecanismo de busca para procurar informações. \\\n",
    "Você tem permissão para fazer múltiplas chamadas (seja em conjunto ou em sequência). \\\n",
    "Procure informações apenas quando tiver certeza do que você quer. \\\n",
    "Se precisar pesquisar alguma informação antes de fazer uma pergunta de acompanhamento, você tem permissão para fazer isso!\n",
    "\"\"\"\n",
    "model_instance = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "tool_instance = TavilySearch(max_results=4)\n",
    "\n",
    "abot = Agent(model_instance, [tool_instance], system=prompt)\n",
    "\n",
    "messages = [HumanMessage(content=\"Como está o tempo em São Paulo hoje?\")]\n",
    "\n",
    "print(\"Iniciando interação do agente:\")\n",
    "final_result_state = None\n",
    "\n",
    "for s in abot.graph.stream({\"messages\": messages}):\n",
    "    print(s)\n",
    "    print(\"---\")\n",
    "    final_result_state = s\n",
    "\n",
    "print(\"\\nResultado Final:\")\n",
    "if final_result_state and 'llm' in final_result_state and final_result_state['llm']['messages']:\n",
    "    print(final_result_state['llm']['messages'][-1].content)\n",
    "else:\n",
    "    print(\"Nenhum resultado final ou resultado inesperado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFLD4ppZvgSm",
    "outputId": "1725274b-89b8-48f6-b5f6-65334a79abed"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "current_date = date.today().strftime(\"%d/%m/%Y\") # Formato dd/mm/aaaa\n",
    "\n",
    "prompt = f\"\"\"Você é um assistente de pesquisa inteligente e altamente atualizado. \\\n",
    "Sua principal prioridade é encontrar as informações mais RECENTES e em TEMPO REAL sempre que possível. \\\n",
    "A data atual é {current_date}. \\\n",
    "Ao buscar sobre o tempo ou eventos que se referem a \"hoje\" ou \"agora\", \\\n",
    "você DEVE **incluir a data atual '{current_date}' na sua consulta para a ferramenta de busca**. \\\n",
    "Por exemplo, se a pergunta é \"tempo em cidade x hoje\", a consulta para a ferramenta deve ser \"tempo em cidade x {current_date}\". \\\n",
    "Ignore ou descarte informações que claramente se refiram a datas passadas ou futuras ao responder perguntas sobre \"hoje\". \\\n",
    "Use o mecanismo de busca para procurar informações, sempre buscando o 'hoje' ou o 'agora' quando o contexto indicar. \\\n",
    "Você tem permissão para fazer múltiplas chamadas (seja em conjunto ou em sequência). \\\n",
    "Procure informações apenas quando tiver certeza do que você quer. \\\n",
    "Se precisar pesquisar alguma informação antes de fazer uma pergunta de acompanhamento, você tem permissão para fazer isso!\n",
    "\"\"\" \n",
    "\n",
    "model_instance = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "tool_instance = TavilySearch(max_results=4)\n",
    "abot = Agent(model_instance, [tool_instance], system=prompt)\n",
    "\n",
    "user_query = \"Como está o tempo em São Paulo hoje?\"\n",
    "\n",
    "messages = [HumanMessage(content=user_query)]\n",
    "\n",
    "print(\"Iniciando interação do agente:\")\n",
    "final_result_state = None\n",
    "for s in abot.graph.stream({\"messages\": messages}):\n",
    "    print(s)\n",
    "    print(\"---\")\n",
    "    final_result_state = s\n",
    "\n",
    "print(\"\\nResultado Final:\")\n",
    "if final_result_state and 'llm' in final_result_state and final_result_state['llm']['messages']:\n",
    "    print(final_result_state['llm']['messages'][-1].content)\n",
    "else:\n",
    "    print(\"Nenhum resultado final ou resultado inesperado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5tKv61m2hPA",
    "outputId": "ace5d86d-24b3-4735-86f6-5036c9098c1b"
   },
   "outputs": [],
   "source": [
    "\n",
    "user_query_tomorrow = \"Como está o tempo em São Paulo amanhã?\" #atualizamos apenas a pergunta do usuário\n",
    "\n",
    "messages_tomorrow = [HumanMessage(content=user_query_tomorrow)]\n",
    "\n",
    "print(\"\\n--- Iniciando interação do agente para amanhã ---\")\n",
    "final_result_state_tomorrow = None\n",
    "for s in abot.graph.stream({\"messages\": messages_tomorrow}):\n",
    "    print(s)\n",
    "    print(\"---\")\n",
    "    final_result_state_tomorrow = s\n",
    "\n",
    "print(\"\\n--- Resultado Final para amanhã ---\")\n",
    "if final_result_state_tomorrow and 'llm' in final_result_state_tomorrow and final_result_state_tomorrow['llm']['messages']:\n",
    "    print(final_result_state_tomorrow['llm']['messages'][-1].content)\n",
    "else:\n",
    "    print(\"Nenhum resultado final ou resultado inesperado para amanhã.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S99lgxim2rIl",
    "outputId": "ad97389a-e337-4d62-aa48-2be1a2601e92"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "user_query_tomorrow = \"Como foi o tempo em São Paulo ontem?\" \n",
    "\n",
    "messages_tomorrow = [HumanMessage(content=user_query_tomorrow)]\n",
    "\n",
    "print(\"\\n--- Iniciando interação do agente ---\")\n",
    "final_result_state_tomorrow = None\n",
    "for s in abot.graph.stream({\"messages\": messages_tomorrow}):\n",
    "    print(s)\n",
    "    print(\"---\")\n",
    "    final_result_state_tomorrow = s\n",
    "\n",
    "print(\"\\n--- Resultado Final  ---\")\n",
    "if final_result_state_tomorrow and 'llm' in final_result_state_tomorrow and final_result_state_tomorrow['llm']['messages']:\n",
    "    print(final_result_state_tomorrow['llm']['messages'][-1].content)\n",
    "else:\n",
    "    print(\"Nenhum resultado final ou resultado inesperado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WqZcDzHsfJI"
   },
   "source": [
    "Então, vamos chamar novamente este agente, perguntando: \"Qual é o tempo em SP hoje? Dessa vez, vamos olhar em detalhes o que o agente buscou e como ele o fez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQKROL8wIsP9",
    "outputId": "9693b8e8-997a-485b-b898-4aac90bef3e4"
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Como está o tempo em São Paulo hoje?\")] \n",
    "result = abot.graph.invoke({\"messages\": messages}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "VIx4DH4oIsLA",
    "outputId": "5e1e8d61-0824-4337-d27e-02413e66dd91"
   },
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CP52888IsIu",
    "outputId": "cc9cbae5-36cb-435e-f601-77c7fd9daad7"
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Como está o tempo em São Paulo e no Rio de Janeiro hoje?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "LHR5mUj6IsGF",
    "outputId": "0750b1bc-a3c2-47c5-a0e9-d83e8a6386a0"
   },
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJ1H0Vu_Ir-6",
    "outputId": "1448e733-3945-4626-801c-180ba99b0fd4"
   },
   "outputs": [],
   "source": [
    "query_passado = \"Qual país sediou a Copa do Mundo de futebol em 1998? Quem foi o campeão e qual o placar da final? \\\n",
    "Qual era o Produto Interno Bruto (PIB) desse país no ano da Copa e qual é o PIB atual (últimos dados disponíveis, como 2023 ou 2024)? \\\n",
    "Qual a capital desse país e qual sua moeda atual? Responda a cada pergunta separadamente.\"\n",
    "messages = [HumanMessage(content=query_passado)]\n",
    "\n",
    "print(\"\\nIniciando interação do agente para pergunta sobre o passado:\")\n",
    "\n",
    "current_state = {}\n",
    "for s in abot.graph.stream({\"messages\": messages}):\n",
    "    current_state.update(s)\n",
    "    print(s)\n",
    "    print(\"---\")\n",
    "\n",
    "print(\"\\n--- Resultado Final para o Passado ---\")\n",
    "if 'llm' in current_state and 'messages' in current_state['llm'] and current_state['llm']['messages']:\n",
    "    final_message_content = current_state['llm']['messages'][-1].content\n",
    "    print(final_message_content)\n",
    "else:\n",
    "    print(\"Nenhum resultado final ou resultado inesperado. Verifique os logs acima.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8cjoraFIr8h",
    "outputId": "ed186763-36fb-475d-fabe-6b54e5308cc8"
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Agente de Pesquisa Interativo ---\")\n",
    "print(\"Digite sua pergunta ou 'sair' para encerrar.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nVocê: \") \n",
    "    if user_input.lower() == \"sair\":\n",
    "        print(\"Agente: Encerrando a conversa. Até logo!\")\n",
    "        break\n",
    "\n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "\n",
    "    print(\"Agente: Pensando e buscando...\")\n",
    "    final_result_state = None\n",
    "    try:\n",
    "\n",
    "        current_state = {}\n",
    "        for s in abot.graph.stream({\"messages\": messages}):\n",
    "            current_state.update(s)\n",
    "\n",
    "        print(\"\\nAgente:\")\n",
    "\n",
    "        if 'llm' in current_state and 'messages' in current_state['llm'] and current_state['llm']['messages']:\n",
    "            final_message = current_state['llm']['messages'][-1]\n",
    "            if hasattr(final_message, 'content'):\n",
    "                print(final_message.content)\n",
    "            else:\n",
    "                print(\"Não foi possível extrair o conteúdo da resposta final do LLM.\")\n",
    "        else:\n",
    "            print(\"Não foi possível obter uma resposta do agente para esta pergunta.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Agente: Ocorreu um erro durante a execução: {e}\")\n",
    "        print(\"Tente novamente ou digite 'sair'.\")\n",
    "\n",
    "print(\"\\n--- Conversa Encerrada ---\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
